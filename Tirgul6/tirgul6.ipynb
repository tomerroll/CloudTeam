{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPwC8Why6h1x"
      },
      "outputs": [],
      "source": [
        "# ×”×ª×§× ×ª ×¡×¤×¨×™×•×ª\n",
        "!pip install nltk\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "# -----------------------------\n",
        "# ×™×‘×•× ×¡×¤×¨×™×•×ª\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# ×”×•×¨×“×•×ª ×©×œ ××©××‘×™× ×œ-NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# -----------------------------\n",
        "# ×©×œ×‘ 1: ×¨×©×™××ª ×”-Stop Words\n",
        "custom_stop_words = set([\n",
        "    'the', 'is', 'at', 'which', 'on', 'and', 'a', 'an', 'to', 'for', 'of',\n",
        "    'with', 'by', 'from', 'in', 'this', 'that', 'it', 'as', 'be', 'are', 'was',\n",
        "    'mqtt', 'org', 'data', 'information', 'protocol', 'message', 'client', 'server'\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# ×©×œ×‘ 2: ×§×™×©×•×¨×™ ×”×“×¤×™×\n",
        "doc_links = [\n",
        "    \"https://en.wikipedia.org/wiki/Battle_of_Carrhae\"\n",
        "]\n",
        "\n",
        "doc_ids_to_links = {}\n",
        "for i, url in enumerate(doc_links):\n",
        "    doc_ids_to_links[f\"doc_{i+1}\"] = url\n",
        "\n",
        "# -----------------------------\n",
        "# ×©×œ×‘ 3: ×”×•×¨×“×ª ×”×˜×§×¡×˜×™× ××›×œ ×“×£\n",
        "docs = {}\n",
        "for i, url in enumerate(doc_links):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        text = soup.get_text(separator=' ')\n",
        "        docs[f\"doc_{i+1}\"] = text.lower()\n",
        "    else:\n",
        "        docs[f\"doc_{i+1}\"] = \"\"\n",
        "\n",
        "# -----------------------------\n",
        "# ×©×œ×‘ 4: ×—×™×©×•×‘ ×©×›×™×—×•×™×•×ª\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "word_counts = Counter()\n",
        "\n",
        "for doc_id, content in docs.items():\n",
        "    words = re.findall(r'\\b[a-z]{2,}\\b', content)\n",
        "    for word in words:\n",
        "        if word in custom_stop_words:\n",
        "            continue\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "        word_counts[lemma] += 1\n",
        "\n",
        "# -----------------------------\n",
        "# ×©×œ×‘ 5: ×‘×—×™×¨×ª 10 ××™×œ×™× ××©××¢×•×ª×™×•×ª\n",
        "top_10 = [word for word, count in word_counts.most_common(10)]\n",
        "print(\"ğŸ” 10 ××™×œ×™× ×©× ×‘×—×¨×•:\", top_10)\n",
        "\n",
        "# -----------------------------\n",
        "# ×©×œ×‘ 6: ×‘× ×™×™×ª ×”××™× ×“×§×¡ ×”×¡×•×¤×™\n",
        "index = {}\n",
        "for doc_id, content in docs.items():\n",
        "    words = re.findall(r'\\b[a-z]{2,}\\b', content)\n",
        "    for word in words:\n",
        "        if word in custom_stop_words:\n",
        "            continue\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "        if lemma not in top_10:\n",
        "            continue\n",
        "        if lemma not in index:\n",
        "            index[lemma] = {'count': 0, 'DocIDs': set()}\n",
        "        index[lemma]['count'] += 1\n",
        "        index[lemma]['DocIDs'].add(doc_id)\n",
        "\n",
        "# -----------------------------\n",
        "# ×©×œ×‘ 7: ×™×¦×™×¨×ª ×˜×‘×œ×”\n",
        "index_data = []\n",
        "for term, data in index.items():\n",
        "    links = [doc_ids_to_links[doc] for doc in data['DocIDs']]\n",
        "    index_data.append({'term': term, 'count': data['count'], 'DocIDs': links})\n",
        "\n",
        "df = pd.DataFrame(index_data)\n",
        "df = df.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
        "print(\"ğŸ“ ××™× ×“×§×¡ ×¡×•×¤×™:\")\n",
        "print(df)\n",
        "\n",
        "# -----------------------------\n",
        "# ×©×œ×‘ 8: ×©×œ×™×—×” ×œ-Firebase\n",
        "database_url = \"https://testtragil6-default-rtdb.firebaseio.com/\"\n",
        "\n",
        "data_to_send = {}\n",
        "for i, row in df.iterrows():\n",
        "    term = row['term']\n",
        "    doc_ids = row['DocIDs']\n",
        "    count = row['count']\n",
        "    data_to_send[term] = {\n",
        "        'term': term,\n",
        "        'DocIDs': doc_ids,\n",
        "        'count': count\n",
        "    }\n",
        "\n",
        "response = requests.put(database_url + \".json\", data=json.dumps(data_to_send))\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"âœ… ×”××™× ×“×§×¡ ×”×•×¢×œ×” ×‘×”×¦×œ×—×” ×œ-Firebase Realtime Database!\")\n",
        "else:\n",
        "    print(\"âŒ ×©×’×™××” ×‘×”×¢×œ××”:\", response.text)\n"
      ]
    }
  ]
}
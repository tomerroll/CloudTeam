{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr8-tuzsHlg7",
        "outputId": "10ac46fb-5dd4-4c42-aa74-edda2af1edfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# ×ž×ª×§×™×Ÿ ××ª ×”×¡×¤×¨×™×™×”\n",
        "!pip install nltk\n",
        "\n",
        "# ×ž×™×™×‘× ××ª ×”×¡×¤×¨×™×™×” ×•×ž×•×¨×™×“ ×ž×©××‘×™×\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from nltk.chat.util import Chat, reflections\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "class DocumentFetcher:\n",
        "    def __init__(self, links):\n",
        "        self.links = links\n",
        "        self.docs = {}\n",
        "        self.doc_ids_to_links = {f\"doc_{i+1}\": link for i, link in enumerate(links)}\n",
        "\n",
        "    def fetch(self):\n",
        "        for i, url in enumerate(self.links):\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                doc_id = f\"doc_{i+1}\"\n",
        "                if response.status_code == 200:\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    text = soup.get_text(separator=' ')\n",
        "                    self.docs[doc_id] = text.lower()\n",
        "                else:\n",
        "                    self.docs[doc_id] = \"\"\n",
        "            except Exception:\n",
        "                self.docs[f\"doc_{i+1}\"] = \"\"\n",
        "        return self.docs, self.doc_ids_to_links\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self, stop_words):\n",
        "        self.stop_words = set(stop_words)\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def process(self, docs):\n",
        "        word_counts = Counter()\n",
        "        for content in docs.values():\n",
        "            words = re.findall(r'\\b[a-z]{2,}\\b', content)\n",
        "            for word in words:\n",
        "                if word in self.stop_words:\n",
        "                    continue\n",
        "                lemma = self.lemmatizer.lemmatize(word)\n",
        "                word_counts[lemma] += 1\n",
        "        return word_counts\n",
        "\n",
        "class Indexer:\n",
        "    def __init__(self, stop_words):\n",
        "        self.stop_words = set(stop_words)\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def build_index(self, docs):\n",
        "        index = {}\n",
        "        for doc_id, content in docs.items():\n",
        "            words = re.findall(r'\\b[a-z]{2,}\\b', content)\n",
        "            for word in words:\n",
        "                if word in self.stop_words:\n",
        "                    continue\n",
        "                lemma = self.lemmatizer.lemmatize(word)\n",
        "                if lemma not in index:\n",
        "                    index[lemma] = {'count': 0, 'DocIDs': set()}\n",
        "                index[lemma]['count'] += 1\n",
        "                index[lemma]['DocIDs'].add(doc_id)\n",
        "        return index\n",
        "\n",
        "class FirebaseUploader:\n",
        "    def __init__(self, db_url):\n",
        "        self.db_url = db_url\n",
        "\n",
        "    def upload(self, index, doc_ids_to_links):\n",
        "        data_to_send = {}\n",
        "        for term, data in index.items():\n",
        "            links = [doc_ids_to_links[doc] for doc in data['DocIDs']]\n",
        "            data_to_send[term] = {\n",
        "                'term': term,\n",
        "                'DocIDs': links,\n",
        "                'count': data['count']\n",
        "            }\n",
        "        try:\n",
        "            response = requests.put(self.db_url + \".json\", data=json.dumps(data_to_send))\n",
        "            return response.status_code == 200, response.text\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "\n",
        "class LogicalSearch:\n",
        "    def __init__(self, index, stop_words):\n",
        "        self.index = index\n",
        "        self.stop_words = set(stop_words)\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess_term(self, term):\n",
        "        term = term.lower()\n",
        "        if term in self.stop_words:\n",
        "            return None\n",
        "        return self.lemmatizer.lemmatize(term)\n",
        "\n",
        "    def search(self, query):\n",
        "        query_upper = query.upper()\n",
        "        operators = re.findall(r'AND|OR', query_upper)\n",
        "        raw_terms = [term for term in query_upper.split() if term not in ('AND', 'OR')]\n",
        "\n",
        "        terms = []\n",
        "        for term in raw_terms:\n",
        "            processed = self.preprocess_term(term)\n",
        "            if processed is None:\n",
        "                continue\n",
        "            terms.append(processed)\n",
        "\n",
        "        if not terms:\n",
        "            print(\"âš ï¸ ××™×Ÿ ×ž×•× ×—×™× ×ª×§×¤×™× ×œ×—×™×¤×•×© ×œ××—×¨ ×¢×™×‘×•×“.\")\n",
        "            return {}\n",
        "\n",
        "        results = []\n",
        "        for term in terms:\n",
        "            if term not in self.index:\n",
        "                print(f\"âš ï¸ ×”×ž×•× ×— '{term}' ×œ× × ×ž×¦× ×‘××™× ×“×§×¡.\")\n",
        "                if 'AND' in operators:\n",
        "                    return {}\n",
        "                continue\n",
        "            results.append(self.index.get(term, {}).get('DocIDs', set()))\n",
        "\n",
        "        if not results:\n",
        "            return {}\n",
        "\n",
        "        result_set = results[0]\n",
        "        for op, next_result in zip(operators, results[1:]):\n",
        "            if op == 'AND':\n",
        "                result_set = result_set & next_result\n",
        "            elif op == 'OR':\n",
        "                result_set = result_set | next_result\n",
        "\n",
        "        ranking = {}\n",
        "        for doc_id in result_set:\n",
        "            score = sum(1 for term in terms if term in self.index and doc_id in self.index[term]['DocIDs'])\n",
        "            ranking[doc_id] = score\n",
        "\n",
        "        return ranking\n",
        "\n",
        "def build_chatbot_patterns(index):\n",
        "    knowledge_base = {\n",
        "        \"mqtt\": \"MQTT is a lightweight messaging protocol for IoT devices, designed for low bandwidth and unreliable networks.\",\n",
        "        \"broker\": \"A broker is a server that receives all messages, filters them, and distributes them to subscribers in MQTT.\",\n",
        "        \"subscribe\": \"Subscribe means a client expresses interest in receiving messages about a specific topic in MQTT.\",\n",
        "        \"publish\": \"Publish means a client sends a message to a topic on the MQTT broker, making it available to subscribers.\",\n",
        "    }\n",
        "\n",
        "    patterns = []\n",
        "    for term in index.keys():\n",
        "        pattern = rf\"(?i).*({term}).*\"\n",
        "        response = knowledge_base.get(term, f\"{term.capitalize()} appears in {len(index[term]['DocIDs'])} document(s).\")\n",
        "        patterns.append((pattern, [response]))\n",
        "    return patterns\n",
        "\n",
        "class SearchEngineCoordinator:\n",
        "    def __init__(self, links, db_url):\n",
        "        self.links = links\n",
        "        self.db_url = db_url\n",
        "        self.stop_words = [\n",
        "            'the', 'is', 'at', 'which', 'on', 'and', 'a', 'an', 'to', 'for', 'of',\n",
        "            'with', 'by', 'from', 'in', 'this', 'that', 'it', 'as', 'be', 'are', 'was',\n",
        "        ]\n",
        "\n",
        "    def start_chatbot(self, index):\n",
        "        print(\"\\nðŸ’¬ Starting Chatbot. Type 'exit' to stop.\")\n",
        "        patterns = build_chatbot_patterns(index)\n",
        "        chatbot = Chat(patterns, reflections)\n",
        "        chatbot.converse()\n",
        "\n",
        "    def run(self):\n",
        "        print(\"â³ Fetching documents...\")\n",
        "        fetcher = DocumentFetcher(self.links)\n",
        "        docs, doc_ids = fetcher.fetch()\n",
        "\n",
        "        print(\"â³ Processing text...\")\n",
        "        processor = TextProcessor(self.stop_words)\n",
        "        processor.process(docs)\n",
        "\n",
        "        print(\"\\nâ³ Building full index (all terms)...\")\n",
        "        indexer = Indexer(self.stop_words)\n",
        "        index = indexer.build_index(docs)\n",
        "\n",
        "        df = pd.DataFrame([\n",
        "            {'term': term, 'count': data['count'], 'DocIDs': [doc_ids[doc] for doc in data['DocIDs']]}\n",
        "            for term, data in index.items()\n",
        "        ])\n",
        "        print(\"\\nðŸ“ Index (sample):\")\n",
        "        print(df.sort_values(by='count', ascending=False).head(20))\n",
        "\n",
        "        print(\"\\nâ³ Uploading index to Firebase...\")\n",
        "        uploader = FirebaseUploader(self.db_url)\n",
        "        success, msg = uploader.upload(index, doc_ids)\n",
        "        if success:\n",
        "            print(\"\\nâœ… Uploaded successfully to Firebase!\")\n",
        "        else:\n",
        "            print(\"\\nâŒ Upload failed:\", msg)\n",
        "\n",
        "        self.start_chatbot(index)\n",
        "\n",
        "        searcher = LogicalSearch(index, self.stop_words)\n",
        "        while True:\n",
        "            query = input(\"\\nðŸ”Ž Enter search query (AND/OR), or 'exit': \")\n",
        "            if query.lower() == 'exit':\n",
        "                break\n",
        "            ranking = searcher.search(query)\n",
        "            if ranking:\n",
        "                print(\"\\nðŸ“„ Matching documents (sorted by relevance):\")\n",
        "                sorted_docs = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
        "                for doc_id, score in sorted_docs:\n",
        "                    print(f\"ðŸ”¹ {doc_ids[doc_id]} (Score: {score})\")\n",
        "            else:\n",
        "                print(\"ðŸ“„ No matching documents found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    doc_links = [\n",
        "        \"https://mqtt.org/\",\n",
        "        \"https://mqtt.org/getting-started/\",\n",
        "        \"https://mqtt.org/mqtt-specification/\",\n",
        "        \"https://mqtt.org/software/\",\n",
        "        \"https://mqtt.org/use-cases/\",\n",
        "        \"https://mqtt.org/faq/\"\n",
        "    ]\n",
        "    firebase_url = \"https://testtragil6-default-rtdb.firebaseio.com/\"\n",
        "\n",
        "    coordinator = SearchEngineCoordinator(doc_links, firebase_url)\n",
        "    coordinator.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ewqd_aIIC5S",
        "outputId": "056d3356-1d90-45d7-fc69-ba894374eeda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Fetching documents...\n",
            "â³ Processing text...\n",
            "\n",
            "â³ Building full index (all terms)...\n",
            "\n",
            "ðŸ“ Index (sample):\n",
            "              term  count                                             DocIDs\n",
            "0             mqtt    357  [https://mqtt.org/faq/, https://mqtt.org/getti...\n",
            "45          client     89    [https://mqtt.org/software/, https://mqtt.org/]\n",
            "91          broker     81  [https://mqtt.org/software/, https://mqtt.org/...\n",
            "82         support     59  [https://mqtt.org/software/, https://mqtt.org/...\n",
            "52         message     41  [https://mqtt.org/software/, https://mqtt.org/...\n",
            "2              iot     35  [https://mqtt.org/software/, https://mqtt.org/...\n",
            "5         protocol     34  [https://mqtt.org/faq/, https://mqtt.org/getti...\n",
            "40             use     34  [https://mqtt.org/faq/, https://mqtt.org/getti...\n",
            "60           cloud     31    [https://mqtt.org/software/, https://mqtt.org/]\n",
            "17          device     30  [https://mqtt.org/software/, https://mqtt.org/...\n",
            "229         source     29                       [https://mqtt.org/software/]\n",
            "186         server     28  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "50             can     28  [https://mqtt.org/software/, https://mqtt.org/...\n",
            "228           open     26                       [https://mqtt.org/software/]\n",
            "38   specification     26  [https://mqtt.org/faq/, https://mqtt.org/getti...\n",
            "293          based     24                       [https://mqtt.org/software/]\n",
            "323    application     22  [https://mqtt.org/faq/, https://mqtt.org/use-c...\n",
            "736           paho     22  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "414        library     22                       [https://mqtt.org/software/]\n",
            "460        eclipse     22  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "\n",
            "â³ Uploading index to Firebase...\n",
            "\n",
            "âœ… Uploaded successfully to Firebase!\n",
            "\n",
            "ðŸ’¬ Starting Chatbot. Type 'exit' to stop.\n",
            ">what is mqtt\n",
            "MQTT is a lightweight messaging protocol for IoT devices, designed for low bandwidth and unreliable networks.\n",
            ">eclipse\n",
            "Ip appears in 3 document(s).\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptfl9TqWLRSO",
        "outputId": "a7816784-4c0a-45ae-d5ba-27092fb2b57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Fetching documents...\n",
            "â³ Processing text...\n",
            "\n",
            "â³ Building full index (all terms)...\n",
            "\n",
            "ğŸ“ Index (sample):\n",
            "              term  count                                             DocIDs\n",
            "0             mqtt    357  [https://mqtt.org/faq/, https://mqtt.org/, htt...\n",
            "45          client     89    [https://mqtt.org/software/, https://mqtt.org/]\n",
            "91          broker     81  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "82         support     59  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "52         message     41  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "2              iot     35  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "5         protocol     34  [https://mqtt.org/, https://mqtt.org/getting-s...\n",
            "40             use     34  [https://mqtt.org/faq/, https://mqtt.org/, htt...\n",
            "60           cloud     31    [https://mqtt.org/software/, https://mqtt.org/]\n",
            "17          device     30  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "229         source     29                       [https://mqtt.org/software/]\n",
            "186         server     28  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "50             can     28  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "228           open     26                       [https://mqtt.org/software/]\n",
            "38   specification     26  [https://mqtt.org/faq/, https://mqtt.org/, htt...\n",
            "293          based     24                       [https://mqtt.org/software/]\n",
            "323    application     22  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "736           paho     22  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "414        library     22                       [https://mqtt.org/software/]\n",
            "460        eclipse     22  [https://mqtt.org/faq/, https://mqtt.org/softw...\n",
            "\n",
            "â³ Uploading index to Firebase...\n",
            "\n",
            "âœ… Uploaded successfully to Firebase!\n",
            "\n",
            "ğŸ” Enter search query (AND/OR), or 'exit': ×‘×©× \n",
            "âš ï¸ ×”××•× ×— '×‘×©×' ×œ× × ××¦× ×‘××™× ×“×§×¡.\n",
            "ğŸ“„ No matching documents found.\n",
            "\n",
            "ğŸ” Enter search query (AND/OR), or 'exit': can\n",
            "\n",
            "ğŸ“„ Matching documents (sorted by relevance):\n",
            "ğŸ”¹ https://mqtt.org/faq/ (Score: 1)\n",
            "ğŸ”¹ https://mqtt.org/software/ (Score: 1)\n",
            "ğŸ”¹ https://mqtt.org/ (Score: 1)\n",
            "\n",
            "ğŸ” Enter search query (AND/OR), or 'exit': can AND paho \n",
            "\n",
            "ğŸ“„ Matching documents (sorted by relevance):\n",
            "ğŸ”¹ https://mqtt.org/software/ (Score: 2)\n",
            "ğŸ”¹ https://mqtt.org/faq/ (Score: 2)\n",
            "\n",
            "ğŸ” Enter search query (AND/OR), or 'exit': can \n",
            "\n",
            "ğŸ“„ Matching documents (sorted by relevance):\n",
            "ğŸ”¹ https://mqtt.org/faq/ (Score: 1)\n",
            "ğŸ”¹ https://mqtt.org/software/ (Score: 1)\n",
            "ğŸ”¹ https://mqtt.org/ (Score: 1)\n",
            "\n",
            "ğŸ” Enter search query (AND/OR), or 'exit': can OR paho\n",
            "\n",
            "ğŸ“„ Matching documents (sorted by relevance):\n",
            "ğŸ”¹ https://mqtt.org/software/ (Score: 2)\n",
            "ğŸ”¹ https://mqtt.org/faq/ (Score: 2)\n",
            "ğŸ”¹ https://mqtt.org/ (Score: 1)\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import json\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# ×”×•×¨×“×ª ××©××‘×™× ×©×œ nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "class DocumentFetcher:\n",
        "    def __init__(self, links):\n",
        "        self.links = links\n",
        "        self.docs = {}\n",
        "        self.doc_ids_to_links = {f\"doc_{i+1}\": link for i, link in enumerate(links)}\n",
        "\n",
        "    def fetch(self):\n",
        "        for i, url in enumerate(self.links):\n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                doc_id = f\"doc_{i+1}\"\n",
        "                if response.status_code == 200:\n",
        "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                    text = soup.get_text(separator=' ')\n",
        "                    self.docs[doc_id] = text.lower()\n",
        "                else:\n",
        "                    self.docs[doc_id] = \"\"\n",
        "            except Exception:\n",
        "                self.docs[f\"doc_{i+1}\"] = \"\"\n",
        "        return self.docs, self.doc_ids_to_links\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self, stop_words):\n",
        "        self.stop_words = set(stop_words)\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def process(self, docs):\n",
        "        word_counts = Counter()\n",
        "        for content in docs.values():\n",
        "            words = re.findall(r'\\b[a-z]{2,}\\b', content)\n",
        "            for word in words:\n",
        "                if word in self.stop_words:\n",
        "                    continue\n",
        "                lemma = self.lemmatizer.lemmatize(word)\n",
        "                word_counts[lemma] += 1\n",
        "        return word_counts\n",
        "\n",
        "class Indexer:\n",
        "    def __init__(self, stop_words):\n",
        "        self.stop_words = set(stop_words)\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def build_index(self, docs):\n",
        "        index = {}\n",
        "        for doc_id, content in docs.items():\n",
        "            words = re.findall(r'\\b[a-z]{2,}\\b', content)\n",
        "            for word in words:\n",
        "                if word in self.stop_words:\n",
        "                    continue\n",
        "                lemma = self.lemmatizer.lemmatize(word)\n",
        "                if lemma not in index:\n",
        "                    index[lemma] = {'count': 0, 'DocIDs': set()}\n",
        "                index[lemma]['count'] += 1\n",
        "                index[lemma]['DocIDs'].add(doc_id)\n",
        "        return index\n",
        "\n",
        "class FirebaseUploader:\n",
        "    def __init__(self, db_url):\n",
        "        self.db_url = db_url\n",
        "\n",
        "    def upload(self, index, doc_ids_to_links):\n",
        "        data_to_send = {}\n",
        "        for term, data in index.items():\n",
        "            links = [doc_ids_to_links[doc] for doc in data['DocIDs']]\n",
        "            data_to_send[term] = {\n",
        "                'term': term,\n",
        "                'DocIDs': links,\n",
        "                'count': data['count']\n",
        "            }\n",
        "        try:\n",
        "            response = requests.put(self.db_url + \".json\", data=json.dumps(data_to_send))\n",
        "            return response.status_code == 200, response.text\n",
        "        except Exception as e:\n",
        "            return False, str(e)\n",
        "\n",
        "class LogicalSearch:\n",
        "    def __init__(self, index, stop_words):\n",
        "        self.index = index\n",
        "        self.stop_words = set(stop_words)\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess_term(self, term):\n",
        "        term = term.lower()\n",
        "        if term in self.stop_words:\n",
        "            return None\n",
        "        return self.lemmatizer.lemmatize(term)\n",
        "\n",
        "    def search(self, query):\n",
        "        query_upper = query.upper()\n",
        "        operators = re.findall(r'AND|OR', query_upper)\n",
        "        raw_terms = [term for term in query_upper.split() if term not in ('AND', 'OR')]\n",
        "\n",
        "        terms = []\n",
        "        for term in raw_terms:\n",
        "            processed = self.preprocess_term(term)\n",
        "            if processed is None:\n",
        "                continue\n",
        "            terms.append(processed)\n",
        "\n",
        "        if not terms:\n",
        "            print(\"âš ï¸ ××™×Ÿ ××•× ×—×™× ×ª×§×¤×™× ×œ×—×™×¤×•×© ×œ××—×¨ ×¢×™×‘×•×“.\")\n",
        "            return {}\n",
        "\n",
        "        results = []\n",
        "        for term in terms:\n",
        "            if term not in self.index:\n",
        "                print(f\"âš ï¸ ×”××•× ×— '{term}' ×œ× × ××¦× ×‘××™× ×“×§×¡.\")\n",
        "                if 'AND' in operators:\n",
        "                    return {}\n",
        "                continue\n",
        "            results.append(self.index.get(term, {}).get('DocIDs', set()))\n",
        "\n",
        "        if not results:\n",
        "            return {}\n",
        "\n",
        "        result_set = results[0]\n",
        "        for op, next_result in zip(operators, results[1:]):\n",
        "            if op == 'AND':\n",
        "                result_set = result_set & next_result\n",
        "            elif op == 'OR':\n",
        "                result_set = result_set | next_result\n",
        "\n",
        "        # ×“×™×¨×•×’ ×œ×¤×™ ×¡×š ×”××•× ×—×™× ×©×ª×•×××™× ×‘×›×œ ××¡××š\n",
        "        ranking = {}\n",
        "        for doc_id in result_set:\n",
        "            score = 0\n",
        "            for term in terms:\n",
        "                if term in self.index and doc_id in self.index[term]['DocIDs']:\n",
        "                    score += 1\n",
        "            ranking[doc_id] = score\n",
        "\n",
        "        return ranking\n",
        "\n",
        "class SearchEngineCoordinator:\n",
        "    def __init__(self, links, db_url):\n",
        "        self.links = links\n",
        "        self.db_url = db_url\n",
        "        self.stop_words = [\n",
        "            'the', 'is', 'at', 'which', 'on', 'and', 'a', 'an', 'to', 'for', 'of',\n",
        "            'with', 'by', 'from', 'in', 'this', 'that', 'it', 'as', 'be', 'are', 'was',\n",
        "        ]\n",
        "\n",
        "    def run(self):\n",
        "        print(\"â³ Fetching documents...\")\n",
        "        fetcher = DocumentFetcher(self.links)\n",
        "        docs, doc_ids = fetcher.fetch()\n",
        "\n",
        "        print(\"â³ Processing text...\")\n",
        "        processor = TextProcessor(self.stop_words)\n",
        "        word_counts = processor.process(docs)\n",
        "\n",
        "        print(\"\\nâ³ Building full index (all terms)...\")\n",
        "        indexer = Indexer(self.stop_words)\n",
        "        index = indexer.build_index(docs)\n",
        "\n",
        "        df = pd.DataFrame([\n",
        "            {'term': term, 'count': data['count'], 'DocIDs': [doc_ids[doc] for doc in data['DocIDs']]}\n",
        "            for term, data in index.items()\n",
        "        ])\n",
        "        print(\"\\nğŸ“ Index (sample):\")\n",
        "        print(df.sort_values(by='count', ascending=False).head(20))\n",
        "\n",
        "        print(\"\\nâ³ Uploading index to Firebase...\")\n",
        "        uploader = FirebaseUploader(self.db_url)\n",
        "        success, msg = uploader.upload(index, doc_ids)\n",
        "        if success:\n",
        "            print(\"\\nâœ… Uploaded successfully to Firebase!\")\n",
        "        else:\n",
        "            print(\"\\nâŒ Upload failed:\", msg)\n",
        "\n",
        "        searcher = LogicalSearch(index, self.stop_words)\n",
        "        while True:\n",
        "            query = input(\"\\nğŸ” Enter search query (AND/OR), or 'exit': \")\n",
        "            if query.lower() == 'exit':\n",
        "                break\n",
        "            ranking = searcher.search(query)\n",
        "            if ranking:\n",
        "                print(\"\\nğŸ“„ Matching documents (sorted by relevance):\")\n",
        "                sorted_docs = sorted(ranking.items(), key=lambda x: x[1], reverse=True)\n",
        "                for doc_id, score in sorted_docs:\n",
        "                    print(f\"ğŸ”¹ {doc_ids[doc_id]} (Score: {score})\")\n",
        "            else:\n",
        "                print(\"ğŸ“„ No matching documents found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    doc_links = [\n",
        "        \"https://mqtt.org/\",\n",
        "        \"https://mqtt.org/getting-started/\",\n",
        "        \"https://mqtt.org/mqtt-specification/\",\n",
        "        \"https://mqtt.org/software/\",\n",
        "        \"https://mqtt.org/use-cases/\",\n",
        "        \"https://mqtt.org/faq/\"\n",
        "    ]\n",
        "    firebase_url = \"https://testtragil6-default-rtdb.firebaseio.com/\"\n",
        "\n",
        "    coordinator = SearchEngineCoordinator(doc_links, firebase_url)\n",
        "    coordinator.run()\n"
      ]
    }
  ]
}